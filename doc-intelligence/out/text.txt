=== [Page 1] ===
ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild
Ahmed Masry **
Megh Thakkar
*
Aayush Bajaj
Aaryaman Kartha
Enamul Hoque"
Shafiq Joty
*York University, Canada
MILA - Quebec AI Institute
Salesforce Research "Nanyang Technological University, Singapore
{masry20, aarykary, enamulh} @yorku.ca
{megh.thakkar, aayush.bajaj} @mila.quebec, sjoty@salesforce.com
arXiv:2407.04172v2 [cs.AI] 4 Nov 2024
Abstract
Given the ubiquity of charts as a data anal-
ysis, visualization, and decision-making tool
across industries and sciences, there has been
a growing interest in developing pre-trained
foundation models as well as general purpose
instruction-tuned models for chart understand-
ing and reasoning. However, existing meth-
ods suffer crucial drawbacks across two crit-
ical axes affecting the performance of chart
representation models: they are trained on
data generated from underlying data tables of
the charts, ignoring the visual trends and pat-
terns in chart images, and use weakly aligned
vision-language backbone models for domain-
specific training, limiting their generalizability
when encountering charts in the wild. We ad-
dress these important drawbacks and introduce
ChartGemma, a novel chart understanding and
reasoning model developed over PaliGemma.
Rather than relying on underlying data ta-
bles, ChartGemma is trained on instruction-
tuning data generated directly from chart im-
ages, thus capturing both high-level trends and
low-level visual information from a diverse set
of charts. Our simple approach achieves state-
of-the-art results across 5 benchmarks span-
ning chart summarization, question answering,
and fact-checking, and our elaborate qualitative
studies on real-world charts show that Chart-
Gemma generates more realistic and factually
correct summaries compared to its contempo-
raries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-
nlp/ChartGemma.
1 Introduction
Language-augmented vision foundation models or
vision-language models (VLMs) have proven to
be effective in tackling numerous real-world multi-
modal tasks such as visual segmentation, caption-
ing, question answering, and generation and edit-
ing (Li et al., 2023; Zhu et al., 2023). Though
these models excel when used for general pur-
pose applications in the wild, they often fail to
tackle tasks that require specialized understanding
and decoding of patterns and visualizations (Han
et al., 2023). An important domain-specific us-
age of VLMs is for understanding and reasoning
over charts, given their ubiquity as a data analy-
sis, visualization, and decision-making tool across
businesses, economies, and scientific fields (Hoque
et al., 2022). This has naturally led to the develop-
ment of more specialized foundation models pre-
trained on massive amounts of structured and often
chart-specific data (Liu et al., 2022; Masry et al.,
2023). These models are, however, trained on a
limited source of resources and focus on a specific
set of tasks, constraining their real-world applica-
bility (Masry et al., 2024).
Developing over the success of instruction-
tuning enabling models to generalize to more tasks
and applications (Ouyang et al., 2022), there have
been attempts at 'instruction-tuning' VLMs to en-
dow them the ability to understand charts in more
realistic and fundamental settings (Meng et al.,
2024). These approaches generally depend on
two crucial factors impacting their effectiveness:
(i) Instruction-tuning dataset - these methods ei-
ther use the underlying data tables from existing
web sources (Masry et al., 2024) or use syntheti-
cally generated data-tables (Han et al., 2023) from
LLMs such as GPT-4 (OpenAI, 2023) to curate
the instruction-tuning data, and (ii) Base model -
the existing methods either use chart-specific pre-
trained models like UniChart (Masry et al., 2023)
or VLMs pre-trained with weak image-text align-
ment such as LLaVA (Li et al., 2023). However,
in existing methods, both these factors have criti-
cal drawbacks impacting their ability to understand
real-world complex charts.
Existing methods are restricted to charts that ei-
ther have an underlying data table or require meth-
ods to extract them from the charts, often with low
** Equal contribution.

=== [Page 2] ===
Chart Image
Summary Generated from the Data Table
Summary Generated from Chart Image
Red-hot job market
Jobs by region As of September
San Francisco
12 mlkon
1.114.300
The Bay Area is approaching its al-time
-East Bay
record for total jobs. although Santa
Clara County could be more than a year
before it reaches such lofty
heights.
1.061.300
Santa Clara
County
1004.800
100
"includes San Mateo and Marin counties
3.7 million
January 2001
September:
36
Record high for Bay Area
employment (3.650.000 jobs.)
15
September 2001
Terrorist attacks lead
to loss of 25.200 jobs
July 2009
June 2000
Largest one
33 Largest one-
month loss -
month gan-
19.600 jobs
January 2004
Low point
=
following the
February 2010
Record low for Bay Area
-
(3.216.800 jobs)
employment (3,119.500 jobs)
2000 01 2 03 04 05 06
Sources: State Employment Development Department, staff research
BAY AREANEWS GROU
The data table shows the population of a city or region from
The chart shows the population milestones of Seattle from 1900 to 2016.
1900 to 2016. The population grew steadily from 100,000 in The population has been steadily increasing over the years, with significant
1900 to 550,000 in 1960. After a slight decline in the 1970s, the
population rebounded and reached 704,352 in 2016.
booms during the Gold Rush Era, the Boeing Boom, the Microsoft Boom,
and the Amazon Boom. The population reached 700,000 for the first time
in 2015. The population peaked at around 600,000 during the Boeing
Boom in the 1960s, before declining slightly during the Boeing Bust in
the 1970s. The population increased again during the Microsoft Boom in
the 1990s and has been steadily increasing ever since. The population of
Seattle reached 704,352 in 2016. The data is based on U.S. Census Bureau
data.
Covid has grown gradually less lethal over the pandemic, mainly due to
immunity, the majority of which has come via vaccines
Evolution of Covid's infection fatality ratio in England, overlaid on levels and sources of immmunity
Â« ol adulti with antibodies
Covid IFR
1.01
People with immunity
via vaccination
vaccinated
Boostery"
0.23
0.13
of adults who have had Covid
2020
2021
Ap
-8
Jan
5032
Could IFR calculated using Of's death cert, mentions and Ones intec
dagrh sert, mentions and DIS infection survey. Method from prior work by Dun Houdon
Sources: ONS Cambridge MRC. FT graphic: John Burs-Murdoch / oubursmurdoch
The data table shows the trend of Covid-19 related metrics
from July 2020 to January 2022. It reveals a decline in the
Infection Fatality Rate (IFR) from 80% in July 2020 to 25%
in January 2022. This coincides with a rise in the percentage
of adults who have had Covid, increasing from 10% in July
2020 to 60% in January 2022. The percentage of adults with
antibodies also shows a significant increase, going from 0% in
July 2020 to 100% in January 2022. These trends suggest a
possible correlation between increased exposure to Covid-19
and a lower IFR, likely due to herd immunity and improved
treatment options.
This chart shows the evolution of Covid's infection fatality ratio in Eng-
land, overlaid on levels and sources of immunity. The chart shows that
the Covid infection fatality ratio has decreased significantly since the
beginning of the pandemic. This is likely due to the increasing levels of
immunity in the population, both from vaccination and from infection.
The chart also shows that the Covid infection fatality ratio has been de-
clining at a faster rate since the introduction of vaccines. It also shows
that the most vulnerable groups have been vaccinated and that this has had
a significant impact on reducing the number of deaths from Covid. The
infection fatality ratio has also decreased with the introduction of boosters.
This demonstrates the impact of the vaccine in helping to protect people
from serious illness and death from Covid.
Table 1: Summaries generated from the same LLM, Gemini Flash 1.5, when using the data table and the chart image,
highlighting the importance of understanding the visual attributes to generate more appropriate chart instructions.
accuracy which are used for instruction-tuning data
generation. These data tables are often incapable of
capturing numerous nuanced details in the complex
charts used in real-world applications (Table 1).
Also, in many scenarios, we are concerned with
representing or understanding general trends in the
charts and not individual data points. On the model
side, existing methods use backbones in which the
vision encoder and LLM are weakly-aligned, ei-
ther due to limited data or architecture, limiting
their generalizability to represent real-world charts.
Instruction-tuning a strongly aligned base VLM
can capture the intricacies among diverse chart ele-
ments and corresponding text more efficiently. We
hypothesize that formulating a simple approach
addressing these drawbacks can lead to an effec-
tive foundation model capable of complex chart
understanding and reasoning in the wild.
We propose ChartGemma, an instruction-tuned
multimodal model for chart understanding and rea-
soning. ChartGemma uses instruction-tuning data
for chart representation learning that is directly
generated from the chart images, capturing more
diverse and relevant information while preserving
complex visual features. This also enables us to uti-
lize a much broader array of charts available across
the web as we are not restricted by the availability
of underlying data tables. ChartGemma develops
The provided data table shows the total number of jobs in the
Bay Area, as well as the number of jobs in San Francisco,
the East Bay, and Santa Clara County, from 2000 to 2026.
Unfortunately, the table only provides data for the year 2000,
with the remaining years missing data for all regions except the
total Bay Area jobs. While we can see the total Bay Area job
growth from 3.3 million in 2000 to a projected 4.0 million in
2026, we lack the specific data to assess job trends within San
The chart shows the job market in the Bay Area from 2000 to 2012. The
Bay Area is approaching its all-time record for total jobs, although Santa
Clara County could be more than a year away before it reaches such lofty
heights. The chart shows that the Bay Area job market experienced a
significant drop during the Great Recession, but has since recovered and
is now at an all-time high. The chart also shows the number of jobs in
different regions of the Bay Area, including San Francisco, East Bay, and
Santa Clara County. The chart is a good indicator of the overall health of
the Bay Area economy.
Seattle population milestones
Last year, Seattle passed the 700,000 mark for the first time. New census data
estimate the city's 2016 population at 704,352.
1,000,000
AMAZON BOOM
BOOM
700,000
750,000
BOEING BOOM
600.000
500,000
500,000
GOLD
400,000
RUSH ERA
-300,000
250.000
200,000
100,0000
1900 1910 1920 1930 1940 2950 1960 1970
1980 1990
2035 2014 2016
Source: U.S. Comsun Burvon
THE SEATTLE TIMES
Vulnerable gros
Covid IFR
July 2005
35.800 jobs
HoutY'E
dot com mettoour
3.566.800 jobs
Total Bay Area jobs
Francisco, East Bay, and Santa Clara County over the years. To
draw meaningful conclusions about job market trends in these
regions, it is necessary to have complete data for all years and
all regions.
over PaliGemma (Beyer et al., 2024) which has
been trained on a much larger alignment dataset.
Since ChartGemma uses PaliGemma as its back-
bone, it is also much smaller than existing chart
understanding models, making it suitable for real-
world applications. We evaluate ChartGemma
across 5 benchmarks spanning chart summariza-
tion, question answering, and fact-checking, ob-
taining state-of-the-art results compared to existing
methods. Our qualitative studies also demonstrate
that ChartGemma produces more faithful and re-
alistic summaries of complex charts as compared
to other methods. Through our elaborate analy-
sis, we put forward ChartGemma as an effective
model capable of understanding and reasoning over
real-world charts. Our main contributions are:
Â· We present ChartGemma, a first-of-its-kind mul-
timodal model instruction-tuned for chart un-
derstanding and reasoning using data directly
generated from chart images.
Â· ChartGemma utilizes a stronger backbone
model and more representative instruction-
tuning data, rendering it effective in tackling ex-
isting benchmarks across chart summarization,
question answering, and fact-checking while be-
ing significantly smaller than its counterparts.
Â· Our extensive quantitative and qualitative stud-

=== [Page 3] ===
W.S. Healthcare Costs ane High Relatine ta Other Courires
Multimodal LLM
Chart Instruction Data
Visual
Chart
Corpus
Gemini
Flash 1.5
What could have caused the
sharp increase ..
ChartGemma
(PaliGemma-3B)
Instruction-
tuning
Explain the distribution of dots in
the chart ..
Figure 1: The instruction-tuning data generation process. Chart images are input into Gemini Flash 1.5, which generates visual
chart instructions used to fine-tune our model, ChartGemma (please refer to Â§ 2).
ies reveal that ChartGemma generates more
faithful and human-like summaries and is ex-
tremely capable in understanding and represent-
ing complex real-world charts in the wild.
2 Chart Instruction Data Generation
In this section, we present the details of generating
our instruction-tuning dataset. We start by curating
a diverse corpus of charts that encompasses a range
of visual styles and elements (Â§ 2.1), and then use
it to generate the visual instruction-tuning data di-
rectly from the charts (Â§ 2.2). We illustrate our data
generation pipeline in Fig. 1.
2.1 Assembling the Chart Corpus
Our chart corpus is assembled using a combination
of various sources across three categories: (i) Syn-
thetically generated charts from sources such as
PlotQA (Methani et al., 2020), (ii) Curated charts
from specialized websites such as Statista which
typically exhibit limited visual diversity, and (iii)
In-the-wild charts harvested from the broader web,
such as WebCharts (Masry et al., 2024), noted for
their extensive stylistic variety. While prior ap-
proaches used accompanying metadata (e.g., titles,
data tables, annotations) to generate instructions
from LLMs (Han et al., 2023; Meng et al., 2024),
our method exclusively utilizes the chart images
themselves for generating instruction-tuning data.
This approach also allows us to bypass the con-
straints imposed by metadata availability. In total,
our corpus consists of 122,857 chart images. We
provide an elaborate breakdown of the chart source
and the statistics across each category in Table 4.
2.2 Visual Chart Instructions
We use chart images directly from the above assem-
bled corpus to generate visual instruction-tuning
data. This enables us to synthesize data that can
train a model to capture not just point information,
but complex trends and relations among the chart
elements. Following Masry et al. (2024), we gener-
ate data across two categories: (i) predefined tasks,
which align with common real-world scenarios and
benchmarks, and (ii) open-ended tasks. For pre-
defined tasks, we generate data for,
1. Chain-of-thought (CoT) involves prompting
the model with complex reasoning questions and
enhances the visual reasoning capabilities of the
model by guiding it through the problem-solving
process in a structured manner.
2. Summarization involves prompting the model
to generate summaries that succinctly capture the
key insights and trends from a chart image that
effectively communicates the primary data narra-
tives.
3. Fact Checking asks the model to determine
whether stated facts are supported or refuted by the
data presented in a chart image. Alongside data
generated from our corpus, we use the training sets
of existing chart fact-checking tasks (Akhtar et al.,
2023a,c) in our instruction-tuning data.
4. Chart-to-Markdown tasks the model with gen-
erating the underlying data tables from a chart im-
age in Markdown format. This approach simplifies
rendering and parsing the tables, enhancing their
accessibility and usability.
5. Program Aided Design (Gao et al., 2022) re-
quires the model to generate executable code that
performs necessary calculations and outputs the fi-
nal answer, delegating complex and challenging
mathematical operations to the code interpreter.
Alongside synthetic data generated from our cor-
pus, we use the Multimodal LLM to create exe-
cutable codes for questions in the training split of
the ChartQA dataset (Masry et al., 2022b), aug-
menting our instruction-tuning data with human-
written questions and their corresponding code.
Open-ended Tasks We enrich our instruction-
tuning data by prompting the Multimodal LLM
to generate a variety of tasks typical in real-world
scenarios. This approach enhances the general-
izability of our models and extends their appli-
cability to diverse real-world settings. Example
open-ended tasks include justifying temporal or

=== [Page 4] ===
Dissatisfaction with Mexico's Direction Conti
Chart
Patches
Vision
Encoder
Linear
Projection
Language Model
Gemma 2B
SigLIP 400M
2003 2007
mdmaunity 92
PEW RESEARCH CENTER
Output
What's the average of last three values
in green graph (round to one decimal)?
8.6
Figure 2: ChartGemma architecture featuring the SigLIP
vision encoder and the Gemma-2B language model. Visual
tokens are depicted in red, prefix tokens in green, and suffix
tokens in yellow. Full attention is applied between visual and
prefix tokens (indicated by black lines), while causal attention
is used for suffix tokens (indicated by purple lines) which are
generated autoregressively.
time-series based trends observed in the chart, de-
scribing the different visual elements such as lines,
colors, and legends represented by the chart, criti-
cally analyzing and comparing visual information,
etc. We present concrete examples in Â§ A.2.
We use Gemini Flash-1.5 (Team et al., 2023)
due to its robust multimodal performance, cost-
effectiveness, and high API rate limits.
2.3 Key Dataset Characteristics
To underscore the distinct innovations of our
dataset relative to prior works, we examine two
critical elements: the visual attributes and the qual-
ity of the chart instructions.
Visual Attributes Our instruction-tuning dataset
features a wide range of instructions that emphasize
the visual attributes of chart images. As illustrated
in Fig. 7 in Appendix A.2, the examples highlight
various visual elements such as lines, shapes, col-
ors, trends, chart types, and positions, all of which
are frequently referenced in real-world scenarios.
These enhance the model's visual reasoning capa-
bilities, enabling real-world applications.
Quality To demonstrate the strength of our ap-
proach in generating high-quality and accurate in-
structions, we evaluated 100 randomly sampled
synthesized instructions. We found that our instruc-
tions accurately reflected the chart content in 82%
of the cases, which is a significant improvement
over the 61% accuracy reported for the ChartIn-
struct dataset (Masry et al., 2024). Additionally, we
observed 8% partially correct answers, similar to
that as reported by ChartInstruct. We attribute this
improvement in quality to our method's reliance on
the chart images, rather than using automatically
generated and often erroneous data tables.
3
Modeling and Methodology
3.1 Architecture
ChartGemma uses PaliGemma (Beyer et al., 2024)
as the backbone architecture, which comprises of
the following two components:
Vision Encoder: SigLIP (Zhai et al., 2023) is
a vision transformer (ViT) encoder . Unlike CLIP-
like ViTs (Radford et al., 2021) which use con-
trastive loss on large batches of image-text pairs,
SigLIP is trained on single image-text pairs inde-
pendently as a binary classification task.
Language Model: Gemma-2B
(Team et al.,
2024) is decoder-only transformer-based (Vaswani
et al., 2017) LLM trained on 3 trillion tokens with a
context length of 8,196 tokens. Its pretraining data
mainly consists of English documents, maths, and
code, making it suitable for chart understanding
tasks requiring strong reasoning capabilities.
We present ChartGemma's architecture in Fig. 2.
The input image is taken in 448x448 resolution and
divided into 14x14 pixel patches, each of which is
fed into the vision encoder as a separate token. The
outputs from the vision encoder are passed through
a linear layer that maps the visual features into
the LLM embedding space. These visual tokens
are then concatenated with the input text embed-
dings and passed to Gemma-2B. Unlike previous
VLLMs (Li et al., 2023) that indiscriminately ap-
ply a causal mask on all image and text tokens,
Gemma-2B applies full attention over the input vi-
sual and text tokens while a causal mask is applied
on the output tokens. This improves the contextual
understanding of the image particularly for repre-
senting complex relationships among objects. We
believe this property provides further advantages
when learning representations for chart images con-
taining numerous nuanced complexities.
3.2 Training Setup
Existing chart VLLMs (Meng et al., 2024) typically
employ a two-stage training approach that requires
an initial step to align the vision encoder and the
LLM for understanding chart features, followed
by instruction-tuning. In contrast, we only use a
single-stage approach where we directly finetune
the backbone model on our instruction-tuning data.
We believe that the first stage is required by cur-
rent methods as the VLLM backbones are aligned
using a limited amount of image-text pairs with

=== [Page 5] ===
Model
#Params
aug.
human
avg.
ChartFC ChartCheck T1 ChartCheck T2
Specialist Chart Models
ChartBERT (Akhtar et al., 2023a)
63.8
Pix2Struct (Lee et al., 2022)
282M
81.6
30.5
56.0
-
-
-
Matcha(Liu et al., 2022)
282M
90.2
38.2
64.2
62.80
61.40
UniChart (Masry et al., 2023)
201M
88.56
43.92
66.24
-
-
-
Closed VLMMs
Gemini Pro (Team et al., 2023)
74.1
65.8
GPT4-V (OpenAI, 2023)
78.5
69.6
Chart VLLMs
ChartLlama (Han et al., 2023)
13B
90.36
48.96
69.66
ChartAssisstant (Meng et al., 2024)
13B
93.90
65.90
79.90
ChartInstruct-Llama2 (Masry et al., 2024)
7B
87.76
45.52
66.64
69.57
70.11
68.80
ChartInstruct-Flan-T5-XL (Masry et al., 2024)
3B
85.04
43.36
64.20
70.27
72.03
73.80
ChartGemma (Ours)
3B
90.80
69.52
80.16
70.33
71.50
74.31
-
-
-
-
-
-
-
-
-
-
-
-
Table 2: Performance on closed-ended generation benchmarks: ChartQA, ChartFC, and ChartCheck. ChartGemma generally
outperforms or matches the performance of all the baselines, while being significantly smaller than them (refer to Â§ 4.2).
restricted styles and diversity. In contrast, our back-
bone, PaliGemma, has been trained end-to-end on
10 billion image-text pairs covering a wide variety
of styles. This makes our model more adaptable
and generalizable to different real-world images
(e.g., charts, infographics, documents). We freeze
the vision encoder and only finetune the LLM dur-
ing instruction-tuning. This helps in reducing the
computational complexity and also improves train-
ing stability given the small batch size used for
instruction-tuning PaliGemma.
4 Experiments, Results, and Analyses
4.1 Experimental Setup
Baselines We compare ChartGemma against
baselines comprising of open-source chart-
specialist models and VLLMs instruction-tuned
on chart data, as well as state-of-the-art closed
source multimodal LLMs.
Chart-specialist
models include
ChartBERT (Akhtar et al.,
2023c), Pix2Struct (Lee et al., 2022), MatCha
(Liu et al., 2022), and UniChart (Masry et al.,
2023). Chart VLLMs include ChartLlaMA (Han
et al., 2023), ChartAssistant (Meng et al., 2024),
and ChartInstruct's (Masry et al., 2024) two
variants with LLaMA2 and Flan-T5-XL. We also
compare ChartGemma against two closed-source
multimodal LLMs, namely Gemini Pro (Team
et al., 2023) and GPT4-V (OpenAI, 2023).
Downstream Tasks We evaluate ChartGemma
on a diverse set of 5 established benchmarks eval-
uating chart representation and reasoning abili-
ties: (i) ChartQA (Masry et al., 2022b) - a fac-
toid chart question answering dataset, (ii) ChartFC
(Akhtar et al., 2023a) and (iii) ChartCheck (Akhtar
et al., 2023b) - chart fact checking datasets, (iv)
OpenCQA (Kantharaj et al., 2022) - an open-
ended chart question answering dataset, and (v)
Chart2Text (Shankar et al., 2022) - a chart sum-
marization dataset. While ChartQA and ChartFC
focus on closed-ended generation, OpenCQA and
Chart2Text evaluate open-ended generation abil-
ities of the models. We also manually curate a
set of 100 charts downloaded from the web com-
pletely unseen by any model. We refer to this set as
'Web' in our results, and use them for comparing
the summarization ability of the models.
Evaluation Metrics Following existing works,
we use relaxed accuracy (RA) for ChartQA, ac-
curacy for ChartFC, and use GPT4 as a judge
for open-ended generation tasks, i.e. Chart2Text,
OpenCQA, and our curated Web set of charts and
measure the informativeness and factual correct-
ness on a scale of 1-5 (Post, 2018).
To ensure the reproducibility of our work, we
present the hyperparameters of our instruction-
tuning and downstream task experiments in Â§ B.1.
All experiments were conducted on a 4 A100 GPUS
(80GB) machine using the JAX framework".
4.2 Performance on closed-ended tasks
We compare the performance of ChartGemma to
the various baselines on the closed-ended tasks,
namely ChartQA and ChartFC, and present the
results in Table 2. We see that Chart VLLMs
are generally the better performing set of mod-
els compared to specialist chart models. Within
1https://github.com/google/jax
-
-
-
-
-
-
-
-
-
-
-
ChartQA
(Relaxed Accuracy)
Chart Fact Checking
(Accuracy)

=== [Page 6] ===
Chart QA
(Relaxed Accuracy)
Chart Fact Checking
(Accuracy)
Model
aug.
human
avg.
ChartFC ChartCheck T1 ChartCheck T2
PaliGemma
68.50
PaliGemma+ChartInstruct
70.24
33.84
52.04
48.58
54.21
51.78
LLaVA+Our dataset
61.12
51.12
56.12
61.28
70.22
70.03
ChartGemma (Ours)
89.44
64.80
77.12
69.95
72.03
73.80
Table 3: Ablation results validating our hypothesis on the
effect of our instruction-tuning data and backbone model on
downstream tasks (refer to Â§ 4.2).
Chart VLLMs, we observe that ChartGemma per-
forms the best on ChartQA in terms of the av-
erage overall performance and on both the syn-
thetic ChartFC and real-world-based ChartCheck
test splits. Particularly, the performance improve-
ments on ChartCheck when using ChartGemma,
which is a zero-shot evaluation, can be attributed to
the fact that our instruction-tuning dataset is specifi-
cally designed to generalize to more realistic charts
encountered in this particular evaluation. We ob-
serve that it is also powerful for its small size of 3
billion parameters, and only lags in performance to
the 13 billion parameter ChartAssistant on the aug-
mented set of ChartQA. The significant improve-
ment of ChartGemma over ChartAssistant on the
human-generated split of ChartQA indicates bet-
ter generalization abilities in understanding more
realistic instructions for complex charts.
Given the state-of-the-art performance of Chart-
Gemma, we next perform a series of ablations to
test our hypothesis on the criticality of having (i)
an instruction-tuning dataset derived from chart im-
ages rather than the underlying data tables, and (ii)
the importance of a strong backbone model.
Effect of the instruction-tuning data To vali-
date the effectiveness of synthesizing instruction-
tuning data directly using the chart images as
compared to using their underlying data tables,
we compare ChartGemma with a version of
PaliGemma instruction-tuned on the dataset pre-
sented in ChartInstruct (Masry et al., 2024), which
was generated using the chart data tables. We
present the results in Table 3. We observe remark-
able improvements when using our instruction-
tuning data compared to the data proposed by
ChartInstruct. The improvements are stark on the
human split of ChartQA, indicating that Chart-
Gemma is very efficient in following real-world
human instructions. The significantly weak per-
formance of ChartGemma when using the dataset
from ChartInstruct is in-line with the observations
of the author mentioning a low (61 %) accuracy
of the synthetically generated instruction-tuning
data (Masry et al., 2024).
ChartInstruct-LLaMA2
ChartGemma
Informativeness
Factual Correctness
4.09
4.36
4.2
45
3.65
3.3
3.60
3.28
2.8
2.68
1.4
1.5
Chart2Text- Chart2Text-
Statista
Pew
Chart2Text- Chart2Text-
Statista
Pew
Figure 3: GPT4 scores (from 1-5, with 5 being the high-
est) on the informativeness and factual correctness of
outputs generated by ChartInstruct-LLaMA2 and Chart-
Gemma (refer to Â§ 4.3).
Effect of the backbone model We probe the ef-
fect of using PaliGemma as the backbone model for
ChartGemma, which has better image-text align-
ment compared to other VLMs, on the downstream
performance. We follow existing works (Han et al.,
2023; Masry et al., 2024) that use LLaVA (Liu
et al., 2023b) as a backbone and train LLaVA-1
with our instruction-tuning data. We compare this
variant (LLaVA+Our dataset) with ChartGemma
in Table 3 and observe that ChartGemma performs
significantly better as compared to using LLaVA as
our backbone. This validates our hypothesis that
initializing our architecture with a strongly aligned
model leads to better char understanding, reason-
ing, and generalization capabilities.
4.3 Performance on open-ended tasks
We next compare the performance of Chart-
Gemma with the baselines on chart understand-
ing and reasoning based open-ended generation
benchmarks, OpenCQA (Kantharaj et al., 2022),
Chart2Text (Shankar et al., 2022), and our curated
'Web' set. We do not use the BLEU (Papineni et al.,
2002) scores for comparison as done by previous
works, due to the numerous criticisms of it as an in-
dicative metric (Callison-Burch et al., 2006; Smith
et al., 2016) and follow the widespread practice
of using strong LLMs as a judge due to their high
agreement with human annotators (Zheng et al.,
2023). We use GPT4 to evaluate the informative-
ness and factual correctness of the outputs gen-
erated by the models and present the scores in
Fig. 32. We see that the outputs generated by Chart-
Gemma are generally scored higher as compared
to ChartInstruct. We particularly see significant im-
provement in the factual correctness of the outputs
of ChartGemma, probably due to the fact that our
instruction-tuning data synthesized using the chart
images captures more complex visual elements and
-
-
71.36
58.26
67.34
3.33
3.22 3.29
2.96
3.09
GPT4 Score
Web
Web
2We show the extended results in Appendix B.3.

=== [Page 7] ===
ChartInstruct-LLaMA2
ChartGemma
4.2
3.79
3.80 3.82
3.59
Human Score
3.18
2.80
2.8
1.4
0
Informativeness
Factual Correctness
Structure
Figure 4: Human evaluation scores on the informative-
ness, factual correctness, and structure of outputs gener-
ated by ChartInstruct-LLaMA2 and ChartGemma.
PaliGemma being strongly aligned leads to better
understanding and reasoning over the charts. Our
findings overall indicate that ChartGemma is able
to produce more informative outputs while also
being factually correct in terms of long-form an-
swering or summarization for the charts.
4.4 Human Evaluation on Summarization
Though using online LLMs like GPT4 as a judge
has been shown to have a high correlation with hu-
man annotation (Zheng et al., 2023), there haven't
been studies on measuring this correlation explic-
itly for chart understanding tasks. Hence, to ensure
our observations, evaluations, and conclusions are
robust, we perform a human study on the manually
curated set of 100 charts, 'Web'. Similar to GPT4
evaluation, we compare the informativeness, fac-
tual correctness, and structure of the outputs gener-
ated by ChartGemma with ChartInstruct-LLaMA2.
We first use ChartInstruct-LLaMA2 and Chart-
Gemma to generate summaries for these samples
in the Web set. We then ask 2 different annota-
tors to rate all the responses based on the above
metrics (informativeness, factual correctness, struc-
ture) from 1-5 (5 being the highest) so we can also
measure agreement between the annotations3. We
present the outputs randomly to the annotators to
prevent any biases towards the models and present
the evaluation results in Fig. 4.
From Fig. 4, we observe that ChartGemma con-
sistently outperforms or matches ChartInstruct-
LLaMA2 on all the metrics, and the findings are in-
line with those observed when using GPT4 for eval-
uation (Section 4.3). We observe that ChartGemma
is equally well structured, yet is more informative
and significantly more factually correct. Better in-
formativeness probably stems from the fact that
ChartGemma is trained on data generated from the
chart images and not just the underlying data tables,
enabling it to learn high level trends and concepts
specific to charts. Furthermore, our instruction-
tuning data and a strong backbone model promote
capturing more complex visual elements of charts,
leading to more factual correctness. Overall, since
our evaluation is performed on charts sampled ran-
domly in the wild from the web, ChartGemma's
strong performance validates its effectiveness as a
strong candidate in understanding and reasoning
over real-world charts.
4.5 Error Analysis and Challenges
We analyzed the outputs of our model, Chart-
Gemma, to understand the shortcomings and areas
for improvement. We have discovered the follow-
ing three patterns of errors.
High Resolution Charts Charts with very large, of-
ten skewed dimensions, present challenges for our
model, which uses an input resolution of 448x448.
Resizing these large images can cause written text
to become unreadable, leading to errors in the pre-
dicted labels and numerical values, as depicted in
Fig. 13. Although PaliGemma offers a variant sup-
porting up to an 896x896 input resolution, it oper-
ates significantly slower than the 448x448 version,
making it impractical for use on consumer-level
machines and GPUs.
Coding Errors While ChartGemma demonstrated
state-of-the-art performance on the ChartQA bench-
mark, excelling in complex numerical reasoning
and compositional questions, it occasionally gen-
erates erroneous code that cannot be executed. As
depicted in Fig. 13, the model sometimes refers to
undeclared variables within the code. We believe
that integrating an LLM with enhanced coding ca-
pabilities could further improve our performance
on the ChartQA benchmark.
Charts with Complex Visual Styles Although
our instruction-tuning corpus predominantly fea-
tures real-world charts from the broad web, Chart-
Gemma tends to exhibit lower factual correctness
and informativeness when evaluated on these charts
compared to those from specialized websites like
Pew or Statista, which have less visual diversity.
This disparity, illustrated in Fig. 3, highlights the
need for further enhancements to improve the gen-
eralizability of chart understanding models across
various visual styles.
4.6 Convergence of ChartGemma
We probe the learning dynamics of ChartGemma
by checking the downstream accuracy with the
3We found a Cohen's Kappa of 0.538 for the agreement.

=== [Page 8] ===
-- ChartQA-Aug
ChartQA-Human
- ChartQA-Average
ChartFC
100
Accuracy
80
60
-
1
2
3
4
5
Number of epochs
Figure 5: Effect of the number of epochs on instruction-
tuning ChartGemma. We observe very quick conver-
gence during training (refer to Â§ 4.6). For ChartQA,
accuracy is relaxed accuracy (Â§ 4.1).
number of instruction-tuning epochs and present
the trends in Fig. 5. We interestingly observe
that ChartGemma converges very quickly, with
the best performance observed at epoch 2. We
attribute this characteristic to the strong alignment
of PaliGemma rendering it effective in adapting
to our relatively generalizable instruction-tuning
dataset. This indicates that PaliGemma is a very
efficient backbone for visual instruction-tuning of
chart data, and might generalize when trained with
a much larger number of samples as well. We leave
this exploration as future work.
5 Related Work
Chart Representation Learning Chart under-
standing models initially were either fine-tuned
from language or vision-language models (Masry
et al., 2022b; Masry and Hoque, 2021; Lee et al.,
2022), or pre-trained using chart-specific learning
objectives (Masry et al., 2023; Liu et al., 2022). Re-
cently, instruction-tuning of pre-trained VLMs has
been explored for enhancing the general applicabil-
ity to charts (Meng et al., 2024; Han et al., 2023;
Masry et al., 2024; Liu et al., 2023a). Though these
methods use diverse sources across the web and
synthetic charts for generating instruction-tuning
data, they utilize the underlying data table of the
charts and train a weakly-aligned backbone VLM.
Chart Modeling Benchmarks With charts be-
ing the standard medium for data visualization
and data-driven decision making, diverse bench-
marks have been proposed to evaluate the abilities
of LLMs and VLMs on chart understanding. These
benchmarks range from close-ended tasks such as
question answering (Methani et al., 2020; Masry
et al., 2022a) to open-ended generation such as ex-
planation generation in OpenCQA (Kantharaj et al.,
2022) and summarization (Shankar et al., 2022).
Chart-specific benchmarks evaluate the ability of
models to convert charts into data tables (Choi et al.,
2019; Masry et al., 2023) or evaluate claims against
given data as a part of general multimodal fact-
checking benchmarks (Akhtar et al., 2023a,c).
Instruction-tuning across modalities and for
charts Instruction-tuning was proposed to gener-
alize the abilities of language models across mul-
tiple tasks (Mishra et al., 2022) and has become a
common practice for adapting pre-trained LLMs
to real-world applications(Alpaca, 2023; Chiang
et al., 2023; Ouyang et al., 2022). The success of
instruction-tuning for text has led to its adoption as
a standard process for multimodal VLMs too (Li
et al., 2023; Zhu et al., 2023; Dai et al., 2023). Re-
cently, domain-specific instruction-tuning has been
attempted for charts that requires specially curated
instruction-tuning data (Han et al., 2023; Masry
et al., 2024; Meng et al., 2024). These methods use
the underlying data tables of the chart to synthesize
the instruction-tuning data. Since the data tables
of charts are not capable of capturing the nuance
details of charts, especially for real-world charts
with complex elements, the instruction-tuning data
generated using the data tables is not adequate for
training models to be adept at understanding these
diverse real-world charts.
6 Conclusion and Future Work
In the landscape of rising excitement for chart
understanding and reasoning models and meth-
ods, we present ChartGemma, a multimodal
model instruction-tuned on data generated di-
rectly from a diverse range of real-world chart
images using a state-of-the-art backbone architec-
ture. ChartGemma addresses two crucial shortcom-
ings of existing instruction-tuned chart models: the
instruction-tuning data is generated from the un-
derlying data tables instead of the chart images,
limiting their adaptability and extendibility to real-
world, and use weakly aligned backbone models,
restricting their generalizability. Our simple ap-
proach yields significant improvements over exist-
ing chart representation models, with a relatively
smaller model in terms of number of parameters.
Our extensive error analyses and human studies
show that ChartGemma produces more realistic,
informative, and factually correct outputs as com-
pared to its contemporaries.
As future work, we aim to formulate a more
diverse instruction-tuning dataset which is created

=== [Page 9] ===
using human written instructions capturing varied
nuances present in charts. We also aim to propose a
more generalized benchmark catered to addressing
complex visual elements in charts with more chart
relevant evaluation metrics.
Limitations
Despite the effectiveness of our instruction-tuning
approach and our model, there are notable limita-
tions. Firstly, the instruction-tuning data is gen-
erated using a proprietary LLM, which could re-
strict the model's use in certain commercial envi-
ronments. Secondly, the input resolution of our
model's vision encoder is capped at 448x448; any
increase in resolution leads to a quadratic rise in
processing time. Third, we depend on the closed-
source model, GPT4, for evaluating crucial metrics
such as Informativeness and Factual Correctness.
The frequent updates and potential deprecation of
closed-source models pose challenges for the repro-
ducibility of our results. Lastly, the model is prone
to hallucinations, occasionally producing factually
incorrect statements or erroneous code. We advise
users to implement robust guardrails and exercise
caution when deploying our model in real-world
applications.
Ethics Statement
Since our model generates responses autoregres-
sively, it is prone to errors and hallucinations. The
outputs can sometimes be misleading or contain
inaccuracies. Additionally, there is no guarantee
that the codes generated by our model will be free
from malicious content. Therefore, it is crucial
for users of our model to implement strict safety
guidelines to mitigate these potential risks. How-
ever, the base datasets we use for further generating
our instruction-tuning data are available publicly
either as full datasets or URLs with public licenses.
Furthermore, all chart images in our dataset were
sourced from existing, publicly available research
papers that have filtered out any offensive content.
We plan to release our visual instruction-tuning
dataset in the same way as the base datasets (images
where the licenses allow us and URLs where they
do not). We also release our trained ChartGemma
model in easy-to-use demos and various formats
and across quantizations for extremely accessible
adoption by the community. For our human evalu-
ation study, we requested the help of our research
collaborators. There were no personal identifica-
tion information collected during this study. As the
focus of the research was about assessing models'
capabilities and limitations in several chart under-
standing tasks, the human evaluation performed
by the authors does not add any ethical issues or
unwanted biases.
Acknowledgements
This research was supported by the Natural Sci-
ences & Engineering Research Council (NSERC)
of Canada and Canada Foundation for Innovation
(CFI). The authors acknowledge the computational
resources provided by the Digital Research Al-
liance of Canada.
References
Mubashara Akhtar, Oana Cocarascu, and Elena Simperl.
2023a. Reading and reasoning over chart images
for evidence-based automated fact-checking. arXiv
preprint arXiv:2301.11843.
Mubashara Akhtar, Oana Cocarascu, and Elena Sim-
perl. 2023b. Reading and reasoning over chart im-
ages for evidence-based automated fact-checking. In
Findings of the Association for Computational Lin-
guistics: EACL 2023, pages 399-414, Dubrovnik,
Croatia. Association for Computational Linguistics.
Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sa-
har Tahmasebi, Oana Cocarascu, and Elena Sim-
perl. 2023c. Chartcheck: An evidence-based fact-
checking dataset over real-world chart images. arXiv
preprint arXiv: 2311.07453.
Alpaca. 2023. Alpaca. https://crfm.stanford.
edu/2023/03/13/alpaca.html.
Lucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto,
Alexander Kolesnikov, Xiao Wang, Daniel Salz,
Maxim Neumann, Ibrahim Alabdulmohsin, Michael
Tschannen, Emanuele Bugliarello, Thomas Un-
terthiner, Daniel Keysers, Skanda Koppula, Fangyu
Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby,
Manoj Kumar, Keran Rong, Julian Eisenschlos,
Rishabh Kabra, Matthias Bauer, Matko BoÅ¡n-
jak, Xi Chen, Matthias Minderer, Paul Voigtlaen-
der, Ioana Bica, Ivana Balazevic, Joan Puigcerver,
Pinelopi Papalampidi, Olivier Henaff, Xi Xiong,
Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.
2024. PaliGemma: A versatile 3B VLM for transfer.
arXiv preprint arXiv:2407.07726.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 249-256, Trento, Italy.
Association for Computational Linguistics.

=== [Page 10] ===
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
J. Choi, Sanghun Jung, Deok Gun Park, J. Choo, and
N. Elmqvist. 2019. Visualizing for the non-visual:
Enabling the visually impaired to use visualization.
Computer Graphics Forum, 38.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. 2023. In-
structblip: Towards general-purpose vision-language
models with instruction tuning.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. arXiv preprint arXiv: 2211.10435.
Yucheng Han, Chi Zhang, Xin Chen, Xu Yang,
Zhibin Wang, Gang Yu, Bin Fu, and Hanwang
Zhang. 2023. Chartllama: A multimodal llm for
chart understanding and generation. arXiv preprint
arXiv:2311.16483.
Enamul Hoque, Parsa Kavehzadeh, and Ahmed Masry.
2022. Chart question answering: State of the art
and future directions. Journal of Computer Graphics
Forum (Proc. EuroVis), pages 555-572.
Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko
Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty.
2022. Opencqa: Open-ended question answering
with charts. In Proceedings of EMNLP (to appear).
Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,
Fangyu Liu, Julian Eisenschlos, Urvashi Khandel-
wal, Peter Shaw, Ming-Wei Chang, and Kristina
Toutanova. 2022. Pix2struct: Screenshot parsing as
pretraining for visual language understanding. arXiv
preprint arXiv:2210.03347.
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto
Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-
mann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-
med: Training a large language-and-vision assis-
tant for biomedicine in one day. arXiv preprint
arXiv:2306.00890.
Fangyu Liu, Francesco Piccinno, Syrine Krichene,
Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin
Altun, Nigel Collier, and Julian Martin Eisenschlos.
2022. Matcha: Enhancing visual language pretrain-
ing with math reasoning and chart derendering. arXiv
preprint arXiv:2212.09662.
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,
Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and
Dong Yu. 2023a. Mmc: Advancing multimodal
chart understanding with large-scale instruction tun-
ing. arXiv preprint arXiv:2311.10774.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023b. Visual instruction tuning. arXiv preprint
arXiv:2304.08485.
Ahmed Masry and Enamul Hoque. 2021. Integrating
image data extraction and table parsing methods for
chart question answering. Chart Question Answering
Workshop, in conjunction with the Conference on
Computer Vision and Pattern Recognition (CVPR),
pages 1-5.
Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Ena-
mul Hoque, and Shafiq Joty. 2023. UniChart: A
universal vision-language pretrained model for chart
comprehension and reasoning. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing (to appear). Association for
Computational Linguistics.
Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty,
and Enamul Hoque. 2022a. ChartQA: A benchmark
for question answering about charts with visual and
logical reasoning. In Findings of the Association for
Computational Linguistics: ACL 2022, pages 2263-
2279, Dublin, Ireland. Association for Computational
Linguistics.
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,
and Enamul Hoque. 2022b. Chartqa: A benchmark
for question answering about charts with visual and
logical reasoning. arXiv preprint arXiv:2203.10244.
Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan
Parvez, Enamul Hoque, and Shafiq Joty. 2024.
Chartinstruct: Instruction tuning for chart compre-
hension and reasoning.
Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao,
Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024. Char-
tassisstant: A universal chart multimodal language
model via chart-to-table pre-training and multitask
instruction tuning. arXiv preprint arXiv:2401.02384.
Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra,
and Pratyush Kumar. 2020. Plotqa: Reasoning over
scientific plots. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision
(WACV).
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 3470-3487, Dublin, Ireland.
Association for Computational Linguistics.
OpenAI. 2023. GPT-4 Technical Report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.

=== [Page 11] ===
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311-318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186-
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural language
supervision.
Kantharaj Shankar, Leong Rixie Tiffany Ko, Lin Xi-
ang, Masry Ahmed, Thakkar Megh, Hoque Enamul,
and Joty Shafiq. 2022. Chart-to-text: A large-scale
benchmark for chart summarization. In In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), 2022.
Aaron Smith, Christian Hardmeier, and Joerg Tiede-
mann. 2016. Climbing mont BLEU: The strange
world of reachable high-BLEU translations. In Pro-
ceedings of the 19th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
269-281.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, and Jiahui Yu
et al. 2023. Gemini: A family of highly capable
multimodal models.
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, LÃ©onard Hussenot,
Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam
Roberts, Aditya Barua, Alex Botev, Alex Castro-
Ros, Ambrose Slone, AmÃ©lie HÃ©liou, Andrea Tac-
chetti, Anna Bulanova, Antonia Paterson, Beth
Tsai, Bobak Shahriari, Charline Le Lan, Christo-
pher A. Choquette-Choo, ClÃ©ment Crepy, Daniel Cer,
Daphne Ippolito, David Reid, Elena Buchatskaya,
Eric Ni, Eric Noland, Geng Yan, George Tucker,
George-Christian Muraru, Grigory Rozhdestvenskiy,
Henryk Michalewski, Ian Tenney, Ivan Grishchenko,
Jacob Austin, James Keeling, Jane Labanowski,
Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
nan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin
Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
Machel Reid, Maciej MikuÅa, Mateo Wirth, Michael
Sharman, Nikolai Chinaev, Nithum Thain, Olivier
Bachem, Oscar Chang, Oscar Wahltinez, Paige Bai-
ley, Paul Michel, Petko Yotov, Rahma Chaabouni,
Ramona Comanescu, Reena Jana, Rohan Anil, Ross
McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,
Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,
Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-
menko, Tom Hennigan, Vlad Feinberg, Wojciech
Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao
Gong, Tris Warkentin, Ludovic Peran, Minh Giang,
ClÃ©ment Farabet, Oriol Vinyals, Jeff Dean, Koray
Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani,
Douglas Eck, Joelle Barral, Fernando Pereira, Eli
Collins, Armand Joulin, Noah Fiedel, Evan Senter,
Alek Andreev, and Kathleen Kenealy. 2024. Gemma:
Open models based on gemini research and technol-
ogy.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998-6008.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
and Lucas Beyer. 2023. Sigmoid loss for language
image pre-training.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
LLM-as-a-judge with MT-bench and chatbot arena.
In Thirty-seventh Conference on Neural Information
Processing Systems Datasets and Benchmarks Track.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592.

=== [Page 12] ===
Appendices
A Chart Instruction Data Generation
A.1 Chart Corpora Collection
We collect chart across 3 categories based on their
source and method of generation as mentioned in
Â§ 2.1. We show the exact statistics and sources
under each category in Table 4.
Sources for instruction-tuning tasks For the
pre-defined tasks used for generating instruction-
tuning data, we also augment the instructions gen-
erated by the multimodal LLM with the training
sets of existing benchmark datasets.
A.2 Instruction Dataset Analysis
Our instruction-tuning dataset comprises of both
closed-ended response generation and open-ended
answering. Fig. 6 shows diverse visual instruction-
tuning tasks that are generally inspired from ex-
isting chart evaluation benchmarks, and Fig. 7
shows diverse visual instruction-tuning tasks in-
spired from open-ended chart understanding and
reasoning.
Instruction-tuning dataset quality As men-
tioned in Â§ 2.3, our instruction-tuning dataset's in-
structions accurately reflect the chart content ap-
proximately 82% of the times, and are partially cor-
rect 8% times. We present some examples where
our instructions are correct and incorrect in Table 5
and partially correct in Table 6.
A.3 Prompt Templates for Instruction-tuning
Data Generation
We present the prompt templates provided to Gem-
ini Flash-1.5 to generate instruction-tuning data
for the program-aided design task in Fig. 8 and
an open-ended task in Fig. 9. Our prompt tem-
plates draw inspiration from the templates used in
ChartInstruct (Masry et al., 2024) and the ChartQA
prompt used in Gemini Flash (Team et al., 2023).
B Experiments and Results
B.1 Hyperparameter settings
We present the hyperparameter settings for
instruction-tuning and fine-tuning on the bench-
marks in Table 7.
B.2 Prompt templates for evaluation
We show the prompt given to GPT4 for evaluating
the outputs of the open-ended tasks, Chart2Text
and our curated 'Web' set for summarization and
OpenCQA in Fig. 10 and Fig. 11, respectively.
B.3 GPT4 evaluation on open-ended
generation tasks
We show the informativeness, factual correctness,
and relevance results on the open-ended genera-
tion tasks, namely Chart2Text(Statista and Pew),
OpenCQA, and our curated 'Web' set of charts in
Table 8.
B.4 Human Evaluation Study
During the human evaluation study, we provided
the human annotators with the same instructions
used to prompt GPT4 as depicted in Fig. 10 and
Fig. 11. We recruited two human volunteers for the
study from our research lab, both were of South-
east Asian (Indian subcontinent) origin and adept
in the English language.
We show the results of human evaluation when
measuring the informativeness, factual correctness,
and structure of outputs generated by ChartInstruct-
LLaMA2 and ChartGemma on the 'Web' set of
charts scraped from the web in Table 9. We
see that ChartGemma significantly outperforms
ChartInstruct-LLaMA2 in terms of informativeness
and factual correctness and they match in the struc-
ture of the generated summary.
B.5 Error Analysis
Fig. 13 show typoes and coding errors produced by
our model.
B.6 Sample Outputs from ChartGemma
In Fig. 14, we provide some sample outputs on
various tasks.

=== [Page 13] ===
Dataset
CoT
Reasoning
Chart
Summarization
Predefined Tasks
Fact
Checking
Chart-to
Markdown
Coding
Abilities
Trend
Analysis
Data
Comparison
Open Ended Tasks
Data
Interpretation
Data
Visualization
Others
#Charts
Synthetic Sources
PlotQA
ChartFC
28000
Specialized Websites
Statista
2688
4996
1296
2377
42098
334
172
373
231
3027
19748
Pew
11951
4999
1251
1784
10034
281
290
30
129
2873
7401
OECD
243
500
644
20838
357
39
47
69
31
489
21712
OWID
717
500
375
2285
1490
40
38
61
28
547
3803
ChartCheck (Wikipedia)
1527
7603
98
96
178
65
1642
1530
General Web
WebCharts
10576
50046
6434
18216
3400
4331
6283
4680
1785
51436
50961
Total
26,175
62,241
45,603
22,603
57,379
792
6926
988
2269
60,014
122,857
Table 4: The number of generated examples for each tasks based on data samples of the mentioned dataset. Some of the charts
are used in multiple tasks. In the last column, we show the number of distinct charts used for instruction generation samples.
(1) Chart Summarization
(2) Coding Abilities
(3) Comparison
Generate a caption for the chart:
The chart titled "Part-time: A Path to Part-time: A path to poverty
Program of thought: What is the
Poverty" compares the poverty rates of " ** (
families with at least one full-time, year-
ALL UL.S.
households
round earner and families with at least
one part-time/part-year earner, in both
all US households and female-headed
households. The poverty rate for all US
households with at least one part-
time/part-year earner
Female-headed
households
and minimum demand for 'Other
advanced flat glass?
Morte and severe weather
cold weather and ion monmns
27.436
90
Extreme heat and withret
1
11.89%
3.0%
demand_2019-965
max_demand=965
min_demand=348
difference=max_demand - min_demand
print (difference)
(4) Chain-of-Thought Reasoning
(5) Fact Checking
(6) Data Extraction
What is the ratio of the score of
"Rep/Lean Rep' to 'Liberal
Democrats'?
Republicans and Democrats have similar levels of
International knowledge
Can following statement be
inferred based on the given
Chart?
" Devestment intentions rise for first time in the pairs
Figure 2- China Integration in Global Vilar Chains, by
The score for 'Rep/Lean Rep' is
6.5 and the score for 'Liberal
Democrats' is 7.2. The ratio of
the score of 'Rep/Lean Rep' to
'Liberal Democrats' is 6.5/7.2 =
0.903. Final Answer: 0.903
Claim: The investment
intentions of firms remained
stagnant from 2005 to 2008.
Verdict: Refutes
The Grubel-Lloyd index for "Precision
Instruments' is approximately 0.7.
(7) Chart-to-Markdown
(8) Data Visualization
(9) Fluctuation Description
Chart to Markdown:
Hours Claw Anbe sidoonengit Kon Arbeit soumets naid Nota furbe if toarert/ngund
Imagine you are creating a
pie chart based on this data.
What would be the dominant
slice and how much of the
pie would it represent?
Describe the fluctuations in
Mayne Pharma's stock price
throughout 2016.
Mayne Pharma
closed down 15
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
The dominant slice in the pie
chart would represent the
white population and would
take up 96% of the pie.
Mayne Pharma's stock price
fluctuated significantly
throughout 2016, with periods of
both increases and decreases
9
-
-
Source: Bloomberg
-
-
-
Compare the number of
major power outages caused
by hurricanes and tropical
storms in 1992 and 2004.
Extreme Weather Is Causing More Major Power Outages
46.896
demand_1999=348
demand_2004 445
demand_2009=514
demand_2014=745
is 27.4%,
compared to 3.8% for households with
at least one full-time, year-round earner.
In female-headed households, the
poverty rate for families with at least
one part-time/part-year earner is 46.8% ..
US Poverty Rata
In 1992, the number of major
power outages caused by
hurricanes and tropical storms
was around 10. In 2004, the
number was around 60.
What is the Grubel-Lloyd index for the
"Precision Instruments' sector?
605
200 6
1
-
mayne pharma
--
Â· unisonai intrmuren
Figure 6: Diverse examples from our visual instruction-tuning tasks that focuses on the visual attributes of the chart
images which are highlighted in green.
difference between the maximum
Spirits Reviving?
Fiscal Year to June 30
5000
12702
5000

=== [Page 14] ===
[1] Data Visualization
[2] Data Visualization
[3] Data Visualization
Describe the shape of the line
representing property investment.
Property slump
China's real estate construction decline
deepened in the second quarter
- Property investment
- Nowystarted pro
- Newly started property projects, area
The line representing property investment
has a generally downward slope, with some
fluctuations, forming a shape similar to a
wave
20
-
Some alternative visualizations include: 1. A
line chart with separate lines for each
category, allowing for a clear comparison of
their trends. 2. A stacked bar chart to show
the combined proportion of respondents in
each category for each time period. 3. A
dual-axis chart to compare the 'Ever done'
category with the 'Past three months'
category on different y-axis scales.",
[4] Data Visualization
[5] Anomaly Detection
[6] Relationship Analysis
If you were to create a different
type of chart to represent this
data, what type of chart would
you choose and why?
A line chart would be a suitable
alternative to the bar chart in this
case. It would clearly highlight the
trend in the number of text
messages sent over time and make it
easier to visualize the growth and
decline periods.
The sharp decline in container
transport by rail in Hungary from
2010 to 2012 stands out as an
unusual point. This is particularly
notable considering the overall
increasing trend in transport
volume for Hungary.'
Yes, there seems to be a correlation. Countries
with higher rankings in 'Starting a business'
generally have higher rankings in 'Enforcing
Contract' as well. This suggests that a more
favorable environment for starting a business
may also be linked to a stronger legal framework
for enforcing contracts."
What's Keeping Investors Away
India ranks lower than mary Asian peers in case of
doing business
Singapore . Malaysia . Thailand . Vietnam = Incia
Overall Fark
Starting a business
Registering Propert
Enforcing Contrael
Soursec Wordt Burku/ Bloomberg
Global Rankings
[7] Time Series Analysis
[8] Data Extrapolation
[9] Data Point Interpretation
How the population makeup changed in the U.S.
Identify the dates when the
black line experiences a
significant drop in power.
Assuming the trend continues,
what could be the possible
ranking of Covid-19 in
September?
What does the topmost layer
of the chart represent?
The black line experiences
significant drops in power
around March 13th and
March 21st.
If the trend continues, the
ranking of Covid-19 as a cause
of death in Wales could be even
lower than 19th in September.
The topmost layer represents
the combined population of
Asian, American Indian, or
Pacific Islander groups.
Notes: Oliver ross ane shown in gray: The Census Bureau added a multiracial option in 2000. Data for nach
group lateint on the chart since then reflect only a single mson or ethnicity.
Figure 7: Diverse examples from our open-ended instruction-tuning tasks that focuses on the visual attributes of the
chart images which are highlighted in green.
Example Prompt - Generate Instruction-tuning data for Program-Aided Design
Generate numerical and visual question-answer pairs for an LLM that we are trying to tune for Chart Numerical and Visual
Reasoning. Your response should be in a json format where each example has three fields: input: which only asks a
numerical/visual question, program of thought: a python program that can be executed to produce the final answer, and
final answer: which is the final answer to the input question based on the chart image.
For the final answer X, follow the following instructions:
* X should contain as few words as possible.
* Don't paraphrase or reformat the text you see in the image.
* If the final answer has two or more items, provide it in the list format like [1, 2].
* When asked to give a ratio, give out the decimal value like 0.25 instead of 1:4.
* When asked to give a percentage, give out the whole value like 17 instead of decimal
like 0.17%
* Don't include any units in the answer.
* Try to include the full label from the graph when asked about an entity.
Generate ten questions that contain some numerical operations such as, but not limited to, max, min, sum, average,
difference, ratio, median, mode, .. etc. Generate another five questions that not only have numerical operations, but also
some visual aspects such as leftmost, rightmost, top, bottom, middle, peak, colors, .. etc. Generate five simple data
retrieval questions that ask about values, x-labels, or legend labels from the chart. Generate another five yes/no
numerical reasoning questions whose answers must be either Yes or No. Generate another four questions that ask to count
some elements in the chart (e.g., the number of bars/pie slices/colors/x-labels) .
Remember that the program of thought must be an executable python code that solves the question step by step and prints
the answer in the end.
Figure 8: Prompt to generate instruction-tuning data for the program-aided design task using Gemini Flash-1.5.
Example Prompt - Generate Instruction-tuning data for Open-ended Tasks
Generate different instruction-tuning tasks for an LLM that we are trying to tune for Chart Understanding. Your response
should be in a json format where each example has three fields: task type, input: which only asks a question or an
instruction related to the task type and the given chart, and expected output: which is the answer to the input
question/instruction based on the input information. Use the following chart image to generate 10 unique tasks
Figure 9: Prompt to generate instruction-tuning data for open-ended tasks using Gemini Flash-1.5.
Suggest an alternative way to
visualize the data that could provide
more insight into the walking and
cycling habits across the countries
What are some alternative ways to
visualize this data to highlight the
differences in the trends between the
two categories?
A scatter plot could be used to visualize the
relationship between the proportion of
people who walk or bicycle 'all of the time'
and the proportion of people who walk or
bicycle 'often' for each country. This could
help identify any patterns or correlations
between the two proportions and provide a
more comprehensive understanding of the
walking and cycling habits across different
countries.
-
-
Source: Chine Nacional Duaswy of Starimicy
-
Identify any unusual or
unexpected data points in the
chart.
Is there a correlation between the ranking
in 'Starting a business' and 'Enforcing
Contract' for the countries shown?

=== [Page 15] ===
Chart Image
Chart Instructions
Answers
Label
Published on TradingView.com.
FX_IDC GBPUSD, D 1.3508 7 -0.0001 (-001%) 0: 3100 Hit 2166 bit kjak Cit.3100
British Pound / U.S. Dollar, Q, IDC
Vol (2)
Based on the chart, what can
you conclude about the per-
The British Pound has depreciated against Correct
the US Dollar over the past six months,
formance of the British Pound with a significant drop between May and
against the US Dollar in the last
six months?
August and a slight recovery towards the
end of the period.
Mare features on tradingview.com
Sep
Based on the trend observed in
the chart, what would you pre-
Based on the trend, the value of the data Correct
at 4:00 PM would likely be slightly lower
dict the value of the data to be than the value at 3:00 PM, possibly around
at 4:00 PM?
41,400.
Change in U.S. Employment:
Current Recession vs. Summary of 10 Postwar Recessions"
Percent change from start of recession
Mildest
Median
Harshest
Current
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36
Source: Fadercl R
Months from start of recession
Bank af Minneapolis
Updated Aux# 2, 2010
"Mildest, mackan, and hanhart linin reflect the secilist, medion, and longest decines cn of
Based on the chart, what might
It is difficult to say for sure based on this Correct
be a possible explanation for the chart alone. However, some potential ex-
high crime rate among native- planations could be socioeconomic factors,
born individuals?
access to resources, or cultural influences.
Immigrant Crime Rates
25)
X. OF EACH GROUP INVOLVED
IN AT LEAST I CRIME IN
THE LAST 12 MONTHS
2- GENERATION
5
1" GENERATION
O
12 15 1+ 15 16 17 1 17 20 21 22 23 24
ALE
Shrinking Giant
Credit Suisse's market cap has dropped heavily and is below Deutsche Bank's
WOwwtiche kaek NG [CHF) - Curent Market Cap & Credit Suisse Group AG (CHF] - Current Market Cap
Compare the market cap of Deutsche Bank and Credit Suisse in 2021. Incorrect
Deutsche Bank and Credit Su- In 2021, Deutsche Bank's market cap
isse in 2021.
was significantly higher than Credit Su-
isse's. Deutsche Bank's market cap was
around 25 billion while Credit Suisse's
was around 15 billion.
U.S. employment picture
Net job gains (in thousands)
Unemployment rate
292,000*
255,000*
4.9%
300
24,000
250
July 2015
200
5.3%
150
100
2
50
0
JASONDJ F MAMJJ
JASONDJ F MAMJJ
2016
*Preliminary
Source: Department of Labor
@latimesgraphics
MLS Home Price index, Vancouver vs. Canadian aggregate
Gouater Vancouver
Â· Canada
260 Index, 2005 = 100
Compare the home price in- In 2015, the home price index in Greater Incorrect
dex in Greater Vancouver and
Canada in 2015.
Vancouver was significantly higher than
in Canada. While Greater Vancouver's
index was around 180, Canada's index was
around 170.
320
200
180
160
140
2017
3013
2014
2015
2016
THE GLOBE AND MAIL, SOURCE- CREA
Describe the trend of job gains
in the chart.
The trend of job gains is generally increas-
ing, with a few dips in between. It experi-
enced a significant peak in July 2016 and
May 2016.
Incorrect
2016
2017
2015
2019
2038
Bloomberg
Searce: Data carepiled by Rasring
408
258
15.1790
10. 1658
0
2016
July 2016
4
10
Based on the trend of the 'Cur- It is difficult to predict the exact percent Correct
rent' recession line, what is the
change after 35 months based solely on
the trend of the 'Current' recession line.
However, based on the upward trend after
28 months, it is likely to be around -5%.
predicted percent change from
the start of the recession after
35 months?
NATIVE ROEN
Table 5: Example answers generated from Gemini Flash 1.5 based on the instructions supplied. We present some
correct generations and some incorrect generations (with highlights in red).
m. December 04. 2019 22:42:00 GMT

=== [Page 16] ===
Chart Image
Chart Instructions
Answers
Label
Time Taken by Top 10 Manufacturers to Upgrade Portfolios to the Latest Android
100%
Version
Counterpoint
Maga Portfolio Updated on Android Pix
50%
Months Afterthe Latest Android Launch
mmung -0=Huawei -0-Xiaomi == OPPO -(-vivo -(-LG -(-Lenave -6- Tacno -- Nokia -+-Alcatel
Identify the dates when the per-
The dates when the percentage was below Partially correct
centage of Londoners moving 30% are: March 3rd, March 7th, March
compared to usual was below
30
11th, March 15th, March 19th, March
23rd, March 27th, 1st May, 5th May, 9th
May, and 13th May. Missing dates
0%
20%
40%
60%
80%
100% 120%
2nd Mar-
6th
10th
14th
18th
22nd
26th
30th
3rd
7th
11th
15th
19th
23rd
27th
1st May
5th
9th
13th
Table 6: Example answers generated from Gemini Flash 1.5 based on the instructions supplied. We present some
partially correct generations here (with highlights in orange).
Example Prompt - Evaluating generated summaries
You will be provided with two summaries generated by different models for chart summarization.
Your task is to evaluate each summary based on three key factors:
Informativeness: How much useful and relevant information from the chart does the summary cover? Does it effectively
convey the main trends and insights?
Factual Correctness: How accurate is the summary in reflecting the information presented in the chart?
Structure: How well-structured is the summary? Does it include an introduction, a body with key insights, and a
conclusion?
You are required to assign a score from 1 to 5 for each factor, for each summary. Please provide your ratings in the
following JSON format :
{
' summary 1' : {
' Informativeness' : score,
'Factual Correctness' : score,
' Structure' : score,
' summary 2' : {
' Informativeness' : score,
'Factual Correctness' : score,
' Structure' : score,
1,
}
Do not return anything else other than the json above.
Figure 10: Example prompt to evaluate open-ended summary generation for Chart2Text and the 'Web' set of charts
using GPT4.
Example Prompt - Evaluating OpenCQA
You will be provided with two answers generated by different models for a question about a chart image.
Your task is to evaluate each answer based on three key factors:
Informativeness: How much useful and relevant information from the chart does the answer cover?
Factual Correctness: How accurate is the answer in reflecting the information presented in the chart?
Relevance: How relevant is the answer to the given question?
You are required to assign a score from 1 to 5 for each factor, for each answer. Please provide your ratings in the
following JSON format :
{
' summary 1' : {
' Informativeness' : score,
'Factual Correctness' : score,
' Relevance' : score,
1,
' summary 2' : {
'Informativeness' : score,
'Factual Correctness' : score,
' Relevance' : score,
},
}}
Figure 11: Example prompt to evaluate open-ended answer generation for OpenCQA using GPT4.
Around 25% of Samsung's portfolio was
portfolio was updated to the updated to the latest Android version after
latest Android version after 9 9 months.
months?
What percentage of Samsung's
Partially correct
% of London moving compared to usual

=== [Page 17] ===
Experiment
# Epochs
Learning Rate Batch Size
Hours
Instruction-tuning
ChartGemma
5
5â¬-5
32
58
Ablations
PaliGemma (chartinstruct)
1
5e-5
32
22
LLaVA + our dataset
1
2e-5
32
11
ChartGemma
1
5e-5
32
11
Finetuning on benchmarks
PaliGemma (ChartFC)
10
5e-5
32
2
PaliGemma (ChartCheck)
10
5e-5
32
4
4
ChartInstruct-LLama2 (ChartCheck)
10
2e-5
32
2
ChartInstruct-Flan-T5-XL (ChartCheck)
10
2e-5
32
1
Table 7: Hyperparameters and training details of our
experiments.
Informativeness Factual Correctness Structure
Statista
ChartInstruct-LLama2
3.33
2.96
3.58
ChartGemma
3.65
3.60
3.66
Pew
ChartInstruct-LLama2
3.38
3.09
3.65
ChartGemma
4.09
4.36
3.85
OpenCQA
ChartInstruct-LLama2
3.54
3.46
4.56
ChartGemma
3.26
3.48
4.19
Web
ChartInstruct-LLama2
ChartGemma
3.22
2.68
3.33
3.29
3.28
3.76
Table 8: GPT4 scores (from 1-5, with 5 being the highest)
on the informativeness and factual correctness of outputs
generated by ChartInstruct-LLaMA2 and ChartGemma (re-
fer to Â§ 4.3).
Chart Gemma
Shifting demographics
and participation rate In Latin America from 2000 to 2028. The working-age
population is represented as a blue line, while the participation rate is shown as
a brown line. The participation rate is measured as a percentage of the working-
age population.
Working-age population and participation rate
The chart shows that the working-age population is projected to peak in 2024.
before declining slightly and then rising again to reach a new peak in 2028. The
participation rate, on the other hand, is projected to continue to rise steadily
throughout the period, reaching a peak of around 73% in 2028.
Forecast:
Participation rate
of working-age population]
The chart highlights the key trend of a growing working-age population in Latin
America, while the participation rate continues to rise. This suggests that the
working-age population is aging, while the participation rate is Increasing. This
trend is likely to have significant implicati
significant implications for economic growth and social
development in the region.
Working-age population
(percent of total population)
The chart depicts the trend of workforce-age population (WAP) and labor
2006
2004
2013
force participation rate (LFPR) in Latin America from 2000 to 2028. The x-axis
represents the years, while the y-os shows the WAP in millions and the
UFPR, as a percentage, The WAP steadily increases from 66 million in 2000 to
74 million in 2028, indicating a growing workforce. In contrast, the LFPR
fluctuates between 66% and 74% over the same period, with a slight decline
Sources IF, World Economic Outlook database; World Bank, Population
IMF
towards the end. This suggests that while the workforce Is expanding, there
Towards the end
is a decrease in the proportion of individual participating in the labor force.
The gap between the two trends highlights the challenge of effectively
integrating the growing workforce into the labor market.
Chart Gemma
Real Weekly Earnings: Year-Over-Year
Receslong
[Pi Adjusted tu Today's Dollar - - - Current: Level w OU%
-
wlevi-seller ving the Omsumer Pier index for Al Weten Omsomen
1
M
The chart also shows that real weekly earnings have been relatively stable in recent years,
with the blue line hovering around 0.3% of the current level. This suggests that the current
level of earnings is close to the long-term average.
Overall, the chart provides a useful overview of the historical trends in real weekly earnings
in the United States. It highlights the impact of recessions and the overall stability of
earnings in recent years.
Chart Instruct:
-
The chart depicts the year-over-year average hourly earnings growth for CPI adjusted and
CPI unaffected workers from 2000 to 2021. The x-axis represents the years, while the y-axis
represents the percentage of earnings growth. CPI adjusted workers consistently had lower
earnings growth compared to OPI unaffected workers, with a maximum growth of 4.19% in
2000 and a minimum growth of -0.3% in 2020 and 2021. In contrast, CPI unaffected workers
had a maximum growth of 4.5% in 2000 and a minimum growth of -0.3% in 2020 and 2021.
Both groups experienced a downward trend in earnings growth from 2020 to 2021. The
Most significant finding is that CPI adjusted workers consistently had lower earnings growth
compared to CPI unaffected workers, with both groups experiencing a decline in earnings
growth in recent years.
Figure 12:
Comparison between ChartGemma and
ChartInstruct-LLama2 for chart captioning.
Informativeness Factual Correctness Structure
ChartInstruct-LLaMA2
3.18
2.80
3.80
ChartGemma
3.79
3.59
3.82
p - value
6.31 Ã 10-6
2.68 Ã 10-7
0.457
Table 9: Human evaluation scores on the informativeness,
factual correctness, and structure of outputs generated by
ChartInstruct-LLaMA2 and ChartGemma. We also provide
the p-values by performing Mann-Whitney U Tests.
The chart shows the real weekly camings in the United States from 2000 to 2024, adjusted
to today's dollar using the Consumer Price Index for All Urban Consumers, The data is
presented as a line graph, with the blue line representing the CPI-adjusted earnings and the
gray shaded areas representing recessions. The chart also includes a horizontal line at 0.3%
representing the current level of earnings.
The chart shows that real weekly earnings have fluctuated over the past two decades, with
periods of both growth and decline. The most notable decline occurred during the Great
Recession of 2008-2009, which is highlighted by a gray shaded area on the chart.
2
The chart titled "Shifting demographics" depicts the working-age population
The share of working-age population is peaking, while the
participation rate continues to rise in Latin America.
Aggregates encompass Brard, Chile, Colombia, Mexico, Peru and Uruguay
Chart Instruct:

=== [Page 18] ===
High Resolution Chart
Coding Error
Complex Visual Chart
program of thought: Who
was the most valuable
player at the 2018 FIFA
World Cup?
......
program of thought: Does
the sum of smallest two
bar is greater than the
value of 9th largest bar?
Generate a
caption for the
chart image:
Kevin De Bryne
-
smallest_two = 0.04 + 0.07
print (smallest_two > 9th_largest)
The chart shows the per capita consumption of poultry and livestock in the US from 1950
to 2020. The data is measured in dollars per capita consumption. The chart shows that
beef consumption has been steadily increasing since 1950, while pork consumption has
been steadily decreasing. Chicken consumption has fluctuated over the years, but has
generally been increasing since 1950. Turkey consumption has remained relatively stable
over the years. The chart also shows that in 1952, chicken overtakes beef as the most
consumed meat in America at 65.49 lbs per year. In 1954, chicken overtakes pork as the
2nd most consumed meat in America at 50.30 lbs per year.
Figure 13: Some samples that our model, ChartGemma, has typos, coding errors, and factually incorrect statements in their
outputs. The errors are shown in red.
Chart Question Answering
What is the trend among
Republicans and Democrats
Fewer Republicans say local news
media are connected to community
% of U.S. adults who say local journalists/news media ...
Are in touch with
Rep/lean Rep
61%
the community
Demylean Dem
More
About the
Les
same
41%
29%
the area where
you live
EU
19
21
China
17
31
Note: No answer responses not shown.
une Suney atus adults conducted A
Source: Survey of U.S. adults conducted April 29-May 5, 2020.
"Americans Give Higher Ratings to South Korea and Germany Than
U.S. for Dealing With Coronavirus"
PEW RESEARCH CENTER
green_bars = [29, 19, 17]
average = sum(green_bars) / len (green_bars)
print (round (average, 1) )
Program Aided Design
Chart Captioning
Generate a caption for the chart image:
The chart shows the number of deaths per million population
due to coronavirus in different countries as of November 2nd,
2022. The data is presented in a stacked bar chart format,
with each bar representing a different country. The bars are
stacked by year, with the top layer representing the number of
deaths in 2023, the middle layer representing the number of
deaths in 2022, the middle layer representing the number of
deaths in 2021, and the bottom layer representing the number
of deaths in 2020. The chart shows that the number of deaths
due to coronavirus has been increasing in all countries since
the beginning of 2020. The highest number of deaths occurred
in Greece, followed by Italy, the USA, and the UK. The lowest
number of deaths occurred in Japan. The chart also shows
that the number of deaths due to coronavirus is increasing in
all countries, with the most significant increase occurring in
Greece.
Coronavirus
ICAEW chart of the week
Desta / million population
Syrian
Libanon
40
4
44
Christiet
Turkey
Source: Spring:2057 Globul Atituden Durvty: Q01.
highest_percentage = 64
lowest_percentage = 3
difference = highest_percentage - lowest_percentage
print (difference)
Program Aided Design
program of thought:
What's the average of all
the values in the green
bars (round to one
decimal)?
Many think China's global influence will
decline after the coronavirus outbreak
% who say each will have _ influence in world affairs
after the coronavirus outbreak compared to before the
outbreak
about the connection
between local journalists
and the communities they
serve?
The
chart
shows
that
a
Source: Survey conducted Oct. 15-Nov. 8, 2018.
PEW RESEARCH CENTER
significantly higher percentage""
of
Democrats
(67%)
believe
local journalists are connected
to their communities compared
to Republicans (61%) .
Jordan, Lebanon, Turkey want fewer Syrian refugees
allowed into their country
In your opinion, should we allow: more refugeesfrom Syria into our country,
faser refugees or about the same as we do nous?'
More at Aboutthe same # Fewer None (VOLI
program of thought: What
is the difference
between the highest
percentage and lowest
percentage?
Mostly cover
49
Have a lot of
influence
Poesadrese of aloolsol use disorders by age, Hans Arda, 0004
--
Figure 14: Sample outputs generated by ChartGemma on various downstream tasks.