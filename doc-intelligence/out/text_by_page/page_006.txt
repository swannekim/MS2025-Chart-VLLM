Chart QA
(Relaxed Accuracy)
Chart Fact Checking
(Accuracy)
Model
aug.
human
avg.
ChartFC ChartCheck T1 ChartCheck T2
PaliGemma
68.50
PaliGemma+ChartInstruct
70.24
33.84
52.04
48.58
54.21
51.78
LLaVA+Our dataset
61.12
51.12
56.12
61.28
70.22
70.03
ChartGemma (Ours)
89.44
64.80
77.12
69.95
72.03
73.80
Table 3: Ablation results validating our hypothesis on the
effect of our instruction-tuning data and backbone model on
downstream tasks (refer to ยง 4.2).
Chart VLLMs, we observe that ChartGemma per-
forms the best on ChartQA in terms of the av-
erage overall performance and on both the syn-
thetic ChartFC and real-world-based ChartCheck
test splits. Particularly, the performance improve-
ments on ChartCheck when using ChartGemma,
which is a zero-shot evaluation, can be attributed to
the fact that our instruction-tuning dataset is specifi-
cally designed to generalize to more realistic charts
encountered in this particular evaluation. We ob-
serve that it is also powerful for its small size of 3
billion parameters, and only lags in performance to
the 13 billion parameter ChartAssistant on the aug-
mented set of ChartQA. The significant improve-
ment of ChartGemma over ChartAssistant on the
human-generated split of ChartQA indicates bet-
ter generalization abilities in understanding more
realistic instructions for complex charts.
Given the state-of-the-art performance of Chart-
Gemma, we next perform a series of ablations to
test our hypothesis on the criticality of having (i)
an instruction-tuning dataset derived from chart im-
ages rather than the underlying data tables, and (ii)
the importance of a strong backbone model.
Effect of the instruction-tuning data To vali-
date the effectiveness of synthesizing instruction-
tuning data directly using the chart images as
compared to using their underlying data tables,
we compare ChartGemma with a version of
PaliGemma instruction-tuned on the dataset pre-
sented in ChartInstruct (Masry et al., 2024), which
was generated using the chart data tables. We
present the results in Table 3. We observe remark-
able improvements when using our instruction-
tuning data compared to the data proposed by
ChartInstruct. The improvements are stark on the
human split of ChartQA, indicating that Chart-
Gemma is very efficient in following real-world
human instructions. The significantly weak per-
formance of ChartGemma when using the dataset
from ChartInstruct is in-line with the observations
of the author mentioning a low (61 %) accuracy
of the synthetically generated instruction-tuning
data (Masry et al., 2024).
ChartInstruct-LLaMA2
ChartGemma
Informativeness
Factual Correctness
4.09
4.36
4.2
45
3.65
3.3
3.60
3.28
2.8
2.68
1.4
1.5
Chart2Text- Chart2Text-
Statista
Pew
Chart2Text- Chart2Text-
Statista
Pew
Figure 3: GPT4 scores (from 1-5, with 5 being the high-
est) on the informativeness and factual correctness of
outputs generated by ChartInstruct-LLaMA2 and Chart-
Gemma (refer to ยง 4.3).
Effect of the backbone model We probe the ef-
fect of using PaliGemma as the backbone model for
ChartGemma, which has better image-text align-
ment compared to other VLMs, on the downstream
performance. We follow existing works (Han et al.,
2023; Masry et al., 2024) that use LLaVA (Liu
et al., 2023b) as a backbone and train LLaVA-1
with our instruction-tuning data. We compare this
variant (LLaVA+Our dataset) with ChartGemma
in Table 3 and observe that ChartGemma performs
significantly better as compared to using LLaVA as
our backbone. This validates our hypothesis that
initializing our architecture with a strongly aligned
model leads to better char understanding, reason-
ing, and generalization capabilities.
4.3 Performance on open-ended tasks
We next compare the performance of Chart-
Gemma with the baselines on chart understand-
ing and reasoning based open-ended generation
benchmarks, OpenCQA (Kantharaj et al., 2022),
Chart2Text (Shankar et al., 2022), and our curated
'Web' set. We do not use the BLEU (Papineni et al.,
2002) scores for comparison as done by previous
works, due to the numerous criticisms of it as an in-
dicative metric (Callison-Burch et al., 2006; Smith
et al., 2016) and follow the widespread practice
of using strong LLMs as a judge due to their high
agreement with human annotators (Zheng et al.,
2023). We use GPT4 to evaluate the informative-
ness and factual correctness of the outputs gen-
erated by the models and present the scores in
Fig. 32. We see that the outputs generated by Chart-
Gemma are generally scored higher as compared
to ChartInstruct. We particularly see significant im-
provement in the factual correctness of the outputs
of ChartGemma, probably due to the fact that our
instruction-tuning data synthesized using the chart
images captures more complex visual elements and
-
-
71.36
58.26
67.34
3.33
3.22 3.29
2.96
3.09
GPT4 Score
Web
Web
2We show the extended results in Appendix B.3.