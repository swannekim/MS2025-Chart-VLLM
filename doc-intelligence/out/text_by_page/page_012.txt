Appendices
A Chart Instruction Data Generation
A.1 Chart Corpora Collection
We collect chart across 3 categories based on their
source and method of generation as mentioned in
ยง 2.1. We show the exact statistics and sources
under each category in Table 4.
Sources for instruction-tuning tasks For the
pre-defined tasks used for generating instruction-
tuning data, we also augment the instructions gen-
erated by the multimodal LLM with the training
sets of existing benchmark datasets.
A.2 Instruction Dataset Analysis
Our instruction-tuning dataset comprises of both
closed-ended response generation and open-ended
answering. Fig. 6 shows diverse visual instruction-
tuning tasks that are generally inspired from ex-
isting chart evaluation benchmarks, and Fig. 7
shows diverse visual instruction-tuning tasks in-
spired from open-ended chart understanding and
reasoning.
Instruction-tuning dataset quality As men-
tioned in ยง 2.3, our instruction-tuning dataset's in-
structions accurately reflect the chart content ap-
proximately 82% of the times, and are partially cor-
rect 8% times. We present some examples where
our instructions are correct and incorrect in Table 5
and partially correct in Table 6.
A.3 Prompt Templates for Instruction-tuning
Data Generation
We present the prompt templates provided to Gem-
ini Flash-1.5 to generate instruction-tuning data
for the program-aided design task in Fig. 8 and
an open-ended task in Fig. 9. Our prompt tem-
plates draw inspiration from the templates used in
ChartInstruct (Masry et al., 2024) and the ChartQA
prompt used in Gemini Flash (Team et al., 2023).
B Experiments and Results
B.1 Hyperparameter settings
We present the hyperparameter settings for
instruction-tuning and fine-tuning on the bench-
marks in Table 7.
B.2 Prompt templates for evaluation
We show the prompt given to GPT4 for evaluating
the outputs of the open-ended tasks, Chart2Text
and our curated 'Web' set for summarization and
OpenCQA in Fig. 10 and Fig. 11, respectively.
B.3 GPT4 evaluation on open-ended
generation tasks
We show the informativeness, factual correctness,
and relevance results on the open-ended genera-
tion tasks, namely Chart2Text(Statista and Pew),
OpenCQA, and our curated 'Web' set of charts in
Table 8.
B.4 Human Evaluation Study
During the human evaluation study, we provided
the human annotators with the same instructions
used to prompt GPT4 as depicted in Fig. 10 and
Fig. 11. We recruited two human volunteers for the
study from our research lab, both were of South-
east Asian (Indian subcontinent) origin and adept
in the English language.
We show the results of human evaluation when
measuring the informativeness, factual correctness,
and structure of outputs generated by ChartInstruct-
LLaMA2 and ChartGemma on the 'Web' set of
charts scraped from the web in Table 9. We
see that ChartGemma significantly outperforms
ChartInstruct-LLaMA2 in terms of informativeness
and factual correctness and they match in the struc-
ture of the generated summary.
B.5 Error Analysis
Fig. 13 show typoes and coding errors produced by
our model.
B.6 Sample Outputs from ChartGemma
In Fig. 14, we provide some sample outputs on
various tasks.